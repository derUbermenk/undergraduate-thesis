{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shoreline change analysis code ###\n",
    "\n",
    "<p>This uses the detected shorelines from the initial steps in coastsat</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.ion()\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_transects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize project ###\n",
    "\n",
    "<p> Setup settings and load shoreline points </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init setttings\n",
    "# region of interest (longitude, latitude)\n",
    "polygon = [[\n",
    "  [123.28654048, 13.89552683],\n",
    "  [123.28654048, 13.90364783],\n",
    "  [123.28977272, 13.90364783],\n",
    "  [123.28977272, 13.89552683],\n",
    "  [123.28654048, 13.89552683]\n",
    "]]\n",
    "\n",
    "# it's recommended to convert the polygon to the smallest rectangle (sides parallel to coordinate axes)       \n",
    "polygon = SDS_tools.smallest_rectangle(polygon)\n",
    "# date range\n",
    "dates = ['1990-01-01', '2022-10-16']\n",
    "# satellite missions ['L5','L7','L8','L9','S2']\n",
    "sat_list = ['L5', 'L7', 'S2']\n",
    "# choose Landsat collection 'C01' or 'C02'\n",
    "collection = 'C01'\n",
    "# name of the site\n",
    "sitename = 'CATIM'\n",
    "# directory where the data will be stored\n",
    "filepath = os.path.join(os.getcwd(), 'data')\n",
    "# put all the inputs into a dictionnary\n",
    "inputs = {'polygon': polygon, 'dates': dates, 'sat_list': sat_list, 'sitename': sitename, 'filepath':filepath,\n",
    "         'landsat_collection': collection}\n",
    "\n",
    "# before downloading the images, check how many images are available for your inputs\n",
    "# SDS_download.check_images_available(inputs);\n",
    "\n",
    "settings = { \n",
    "    # general parameters:\n",
    "    'cloud_thresh': 0.5,        # threshold on maximum cloud cover\n",
    "    'dist_clouds': 100,         # ditance around clouds where shoreline can't be mapped\n",
    "    'output_epsg': 3124,       # epsg code of spatial reference system desired for the output\n",
    "    # quality control:\n",
    "    'check_detection': False,    # if True, shows each shoreline detection to the user for validation\n",
    "    'adjust_detection': False,  # if True, allows user to adjust the postion of each shoreline by changing the threhold\n",
    "    'save_figure': True,        # if True, saves a figure showing the mapped shoreline for each image\n",
    "    # [ONLY FOR ADVANCED USERS] shoreline detection parameters:\n",
    "    'min_beach_area': 2000,     # minimum area (in metres^2) for an object to be labelled as a beach\n",
    "    'min_length_sl': 500,       # minimum length (in metres) of shoreline perimeter to be valid\n",
    "    'cloud_mask_issue': False,  # switch this parameter to True if sand pixels are masked (in black) on many images  \n",
    "    'sand_color': 'default',    # 'default', 'latest', 'dark' (for grey/black sand beaches) or 'bright' (for white sand beaches)\n",
    "    'pan_off': False,           # True to switch pansharpening off for Landsat 7/8/9 imagery\n",
    "    # add the inputs defined previously\n",
    "    'inputs': inputs,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 duplicates\n",
      "9 bad georef\n"
     ]
    }
   ],
   "source": [
    "# load shoreline points\n",
    "filepath = os.path.join(inputs['filepath'], sitename)\n",
    "with open(os.path.join(filepath, sitename + '_output' + '.pkl'), 'rb') as f:\n",
    "    output = pickle.load(f)\n",
    "# remove duplicates (images taken on the same date by the same satellite)\n",
    "output = SDS_tools.remove_duplicates(output)\n",
    "# remove inaccurate georeferencing (set threshold to 10 m)\n",
    "output = SDS_tools.remove_inaccurate_georef(output, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392 transects have been loaded\n"
     ]
    }
   ],
   "source": [
    "# load transects\n",
    "geojson_file = os.path.join(os.getcwd(), 'cagliliog2.geojson')\n",
    "transects = SDS_tools.transects_from_geojson(geojson_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate intersection points ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate intersection points\n",
    "settings_transects = { # parameters for computing intersections\n",
    "                      'along_dist':          25,        # along-shore distance to use for computing the intersection\n",
    "                      'min_points':          3,         # minimum number of shoreline points to calculate an intersection # dati 3\n",
    "                      'max_std':             15,        # max std for points around transect\n",
    "                      'max_range':           20,        # max range for points around transect # dati 30\n",
    "                      'min_chainage':        0,      # largest negative value along transect (landwards of transect origin)\n",
    "                      'multiple_inter':      'auto',    # mode for removing outliers ('auto', 'nan', 'max')\n",
    "                      'prc_multiple':         0.1,      # percentage of the time that multiple intersects are present to use the max\n",
    "                     }\n",
    "cross_distance= SDS_transects.compute_intersection_QC(output, transects, settings_transects) \n",
    "\n",
    "# save a .csv file for Excel users\n",
    "out_dict = dict([])\n",
    "out_dict['dates'] = output['dates']\n",
    "for key in transects.keys():\n",
    "    out_dict[key] = cross_distance[key]\n",
    "df = pd.DataFrame(out_dict)\n",
    "fn = os.path.join(settings['inputs']['filepath'],settings['inputs']['sitename'],\n",
    "                  'ts.csv')\n",
    "df.to_csv(fn, sep=',')\n",
    "print('Time-series of the shoreline change along the transects saved as:\\n%s'%fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tidal correction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting closest points: 100%Tidally-corrected time-series of the shoreline change along the transects saved as:\n",
      "/home/chester/Documents/Thesis/Cagliliog2/CoastSat/data/CATIM/ts_tidally_corrected.csv\n"
     ]
    }
   ],
   "source": [
    "# load the measured tide data\n",
    "filepath = os.path.join(os.getcwd(),'cagliliog_tides.csv')\n",
    "tide_data = pd.read_csv(filepath, parse_dates=['dates'])\n",
    "dates_ts = [_.to_pydatetime() for _ in tide_data['dates']]\n",
    "tides_ts = np.array(tide_data['tide'])\n",
    "\n",
    "# get tide levels corresponding to the time of image acquisition\n",
    "dates_sat = output['dates']\n",
    "tides_sat = SDS_tools.get_closest_datapoint(dates_sat, dates_ts, tides_ts)\n",
    "\n",
    "# tidal correction along each transect\n",
    "reference_elevation = 0 # elevation at which you would like the shoreline time-series to be\n",
    "beach_slope = 1.990542\n",
    "cross_distance_tidally_corrected = {}\n",
    "for key in cross_distance_tidally_corrected.keys():\n",
    "    correction = (tides_sat-reference_elevation)/beach_slope\n",
    "    cross_distance_tidally_corrected[key] = cross_distance_tidally_corrected[key] + correction\n",
    "    \n",
    "# store the tidally-corrected time-series in a .csv file\n",
    "out_dict = dict([])\n",
    "out_dict['dates'] = dates_sat\n",
    "for key in cross_distance_tidally_corrected.keys():\n",
    "    out_dict[key] = cross_distance_tidally_corrected[key]\n",
    "df = pd.DataFrame(out_dict)\n",
    "fn = os.path.join(settings['inputs']['filepath'],settings['inputs']['sitename'],\n",
    "                  'ts_tidally_corrected.csv')\n",
    "df.to_csv(fn, sep=',')\n",
    "print('Tidally-corrected time-series of the shoreline change along the transects saved as:\\n%s'%fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1992-06-20 01:28:21+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1992-07-22 01:28:22+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1992-08-23 01:27:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1993-06-07 01:27:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1993-11-14 01:27:01+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>554</td>\n",
       "      <td>2022-09-21 02:23:54+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>555</td>\n",
       "      <td>2022-09-26 02:23:47+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>556</td>\n",
       "      <td>2022-10-01 02:23:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>2022-10-06 02:23:44+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>558</td>\n",
       "      <td>2022-10-11 02:23:52+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>559 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                      dates\n",
       "0             0  1992-06-20 01:28:21+00:00\n",
       "1             1  1992-07-22 01:28:22+00:00\n",
       "2             2  1992-08-23 01:27:30+00:00\n",
       "3             3  1993-06-07 01:27:36+00:00\n",
       "4             4  1993-11-14 01:27:01+00:00\n",
       "..          ...                        ...\n",
       "554         554  2022-09-21 02:23:54+00:00\n",
       "555         555  2022-09-26 02:23:47+00:00\n",
       "556         556  2022-10-01 02:23:53+00:00\n",
       "557         557  2022-10-06 02:23:44+00:00\n",
       "558         558  2022-10-11 02:23:52+00:00\n",
       "\n",
       "[559 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = os.path.join(settings['inputs']['filepath'],settings['inputs']['sitename'],\n",
    "                  'ts_.csv')\n",
    "ts_tdc = pd.read_csv(fn)\n",
    "ts_tdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### despiking ###\n",
    "<p>this step removes outliers and despikes the intersection points</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'T0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'T0'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m cross_distance_tidally_corrected \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m([])\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m transects\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m----> 6\u001b[0m     cross_distance_tidally_corrected[key] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(df[key])\n\u001b[1;32m      8\u001b[0m \u001b[39m# remove outliers in the time-series (coastal despiking)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m settings_outliers \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mmax_cross_change\u001b[39m\u001b[39m'\u001b[39m:   \u001b[39m40\u001b[39m,             \u001b[39m# maximum cross-shore change observable between consecutive timesteps\u001b[39;00m\n\u001b[1;32m     10\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39motsu_threshold\u001b[39m\u001b[39m'\u001b[39m:     [\u001b[39m-\u001b[39m\u001b[39m.5\u001b[39m,\u001b[39m0\u001b[39m],        \u001b[39m# min and max intensity threshold use for contouring the shoreline\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mplot_fig\u001b[39m\u001b[39m'\u001b[39m:           \u001b[39mFalse\u001b[39;00m,           \u001b[39m# whether to plot the intermediate steps\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                     }\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'T0'"
     ]
    }
   ],
   "source": [
    "filepath = os.path.join(os.getcwd(),'data', 'CATIM', 'ts_tidally_corrected.csv')\n",
    "df = pd.read_csv(filepath, parse_dates=['dates'])\n",
    "dates = [_.to_pydatetime() for _ in df['dates']]\n",
    "cross_distance_tidally_corrected = dict([])\n",
    "for key in transects.keys():\n",
    "    cross_distance_tidally_corrected[key] = np.array(df[key])\n",
    "\n",
    "# remove outliers in the time-series (coastal despiking)\n",
    "settings_outliers = {'max_cross_change':   40,             # maximum cross-shore change observable between consecutive timesteps\n",
    "                     'otsu_threshold':     [-.5,0],        # min and max intensity threshold use for contouring the shoreline\n",
    "                     'plot_fig':           False,           # whether to plot the intermediate steps\n",
    "                    }\n",
    "\n",
    "cross_distance_despiked = SDS_transects.reject_outliers(cross_distance_tidally_corrected,output,settings_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the tidally-corrected time-series in a .csv file\n",
    "out_dict = dict([])\n",
    "out_dict['dates'] = dates_sat\n",
    "for key in cross_distance_despiked.keys():\n",
    "    out_dict[key] = cross_distance_despiked[key]\n",
    "df = pd.DataFrame(out_dict)\n",
    "fn = os.path.join(settings['inputs']['filepath'],settings['inputs']['sitename'],\n",
    "                  'ts_despiked.csv')\n",
    "df.to_csv(fn, sep=',')\n",
    "print('Despiked time-series of the shoreline change along the transects saved as:\\n%s'%fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post processing ###\n",
    "remove rows where there are large number of missing transect intersects and impute remaining nan values\n",
    "also preprocess the shorelines csv to match transect time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_maker = lambda date: date.strftime('%d/%m/%Y')\n",
    "\n",
    "sls = pd.read_csv('shorelines.csv', parse_dates=['date'])\n",
    "ts_despiked = pd.read_csv('ts_despiked.csv', index_col=0, parse_dates=['dates'])\n",
    "\n",
    "sls['id'] = sls.date.apply(id_maker)\n",
    "ts_despiked['id'] = ts_despiked.dates.apply(id_maker)\n",
    "ts_despiked['dates_parsed'] = ts_despiked['dates']\n",
    "ts_despiked['dates'] = ts_despiked['id']\n",
    "\n",
    "print('ts_despiked: ', ts_despiked.shape)\n",
    "print('sls: ', sls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows based on the given ids, since s2 rows come after l7 and l7 before l5, this ensures\n",
    "# ... we keep last to ensure that the most accurate shoreline position is kept\n",
    "\n",
    "sls.drop_duplicates(subset=['id'], keep='last', inplace=True)\n",
    "ts_despiked.drop_duplicates(subset=['id'], keep='last', inplace=True)\n",
    "\n",
    "print('ts_despiked: ', ts_despiked.shape)\n",
    "print('sls: ', sls.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop nan heavy columns in preprocessed_ts ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan heavy columns in preprocessed_ts\n",
    "percent = 0.95\n",
    "total_columns = len(ts_despiked.columns[1:-2])\n",
    "thresh = int(round(total_columns * 0.95, 0))\n",
    "ts_despiked_nan_dropped = ts_despiked.dropna(thresh=thresh)\n",
    "\n",
    "print('''\n",
    "  delete when {}% of intersects are missing\n",
    "  which is {} transects.\n",
    "'''.format(percent, thresh))\n",
    "print('ts_despiked_nan_dropped shape: ', ts_despiked_nan_dropped.shape)\n",
    "print('''\n",
    "  oldest date: {},\n",
    "  newest date: {}\n",
    "'''.format(\n",
    "  min(ts_despiked_nan_dropped.dates_parsed),\n",
    "  max(ts_despiked_nan_dropped.dates_parsed)\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill out remaining nan values ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_interpolated = ts_despiked_nan_dropped\n",
    "ts_interpolated = ts_interpolated.set_index(['dates_parsed']).interpolate(method='time', limit_direction='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge sls and ts_despiked by id ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sls = sls[['geoaccuracy', 'id']]\n",
    "preprocessed_ts = ts_interpolated\n",
    "ts_sls = preprocessed_ts.set_index('id').join(preprocessed_sls.set_index('id'))\n",
    "\n",
    "\n",
    "print('sls shape', preprocessed_sls.shape)\n",
    "print('ts shape', preprocessed_ts.shape)\n",
    "print('ts_sls shape', preprocessed_ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create new processed transect time series and shorelines ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_processed = ts_sls[ts_sls.columns[0:398]]\n",
    "\n",
    "sls_processed = ts_sls[['dates', 'geoaccuracy']]\n",
    "sls_processed = sls_processed.rename(columns={'dates': 'Date', 'geoaccuracy': 'Uncertainty'})\n",
    "\n",
    "ts_processed.to_csv('ts_despiked_processed.csv', index=False)\n",
    "sls_processed.to_csv('shorelines_processed.csv', index=False)\n",
    "\n",
    "print('''\n",
    "  ts_processed nan count:\n",
    "  sls_processed nan count:\n",
    "'''.format(\n",
    "  ts_processed.isna().sum().sum(),\n",
    "  sls_processed.isna().sum().sum()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('coastsat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ab2a9d622cdcd0a1630d8763ef22a823690475f4099835b27e99b7551097528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
